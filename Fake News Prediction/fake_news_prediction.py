# -*- coding: utf-8 -*-
"""Fake News Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NYPfmZGAYKZ6cpDGI2hHSPI3qZd1FjQc

**train.csv:** <br>
A full training dataset with the following attributes:

**id:** unique id for a news article
**title:** the title of a news article
**author:** author of the news article
**text:** the text of the article; could be incomplete
**label:** a label that marks the article as potentially unreliable

*   1: unreliable
*   0: reliable

**test.csv:** <br>
A testing training dataset with all the same attributes at train.csv without the label.

**submit.csv:** <br>
A sample submission that you can

Source:
https://www.kaggle.com/competitions/fake-news/data
"""

import numpy as np
import pandas as pd
import re  # regular expression: for searching the text
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer  # stemming: take word, remove prefix & suffix, return word.
from sklearn.feature_extraction.text import TfidfVectorizer  # covert word to feature vectors
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk

nltk.download('stopwords')

# printing the stopwords in English
print(stopwords.words('english'))  # etc. print(stopwords.words('chinese'))

"""Data Pre-processing"""

# loading the dataset to a pandas DataFrame
news_dataset = pd.read_csv('train.csv')

print(news_dataset.shape)

# print the first 5 rows of the dataframe
print(news_dataset.head())

# counting the number of missing values in the dataset
print(news_dataset.isnull().sum())

# replacing the null values with empty string
news_dataset = news_dataset.fillna('')

# counting the number of missing values in the dataset
print(news_dataset.isnull().sum())

# merging the author name and news title
news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']

print(news_dataset['content'])

# separating the data & label
X = news_dataset.drop(columns='label', axis=1)
Y = news_dataset['label']

print(X)
print(Y)

"""**Stemming**
Stemming is the process of reducing a word to its Root word

example:
actor,actress,acting --> act
"""

port_stem = PorterStemmer()


def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]', ' ',
                             content)  # ^a-zA-Z exclude everything not alphabets (from content), ' ' means any replacement? by ' '
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content


news_dataset['content'] = news_dataset['content'].apply(stemming)

print(news_dataset['content'])

# Separating data and label
X = news_dataset['content'].values
Y = news_dataset['label'].values

print(X)

# convert textual data to numerical data
vectorizer = TfidfVectorizer()  # Counter the no. of time a particular word repeating, assign a value-represent importance
vectorizer.fit(X)

X = vectorizer.transform(X)

print(X)

"""Splitting dataset to training and test data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

"""Training Model"""

model = LogisticRegression()

model.fit(X_train, Y_train);

"""Evaluation"""

# accuracy score for training data
X_train_prediction = model.predict(X_train)
train_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print(train_data_accuracy)

# accuracy score for testing data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print(test_data_accuracy)

# test.csv
test = pd.read_csv('test.csv')
submit = pd.read_csv('submit.csv')

test.isnull().sum()

test = test.fillna('')

test['content'] = test['author'] + ' ' + test['title']

test['content'] = test['content'].apply(stemming)

test_case = test['content'].values
test_ans = submit['label'].values

print(X)
print(Y)

test_case = vectorizer.transform(test_case)  # unseen test data, dont .fit()

print(test_case.shape)

test_case_prediction = model.predict(test_case)

accuracy = accuracy_score(test_case_prediction, test_ans)
print(accuracy)

"""Make a Predictive System"""

X_new = test_case[0]

prediction = model.predict(X_new)
print(prediction)

if (prediction[0] == 0):
    print("reliable")
else:
    print("unreliable")
